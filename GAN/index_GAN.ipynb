{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 7>DELE ST1504 CA2 Part A: Generative Adversarial Network </font>\n",
    "<hr>\n",
    "<font size = 4>\n",
    "Name: Lee Hong Yi & Yadanar Aung<br>\n",
    "Admin No: 2223010 & 2214621<br>\n",
    "Class: DAAA/FT/2B/07<br>\n",
    "</font>\n",
    "<hr>\n",
    "\n",
    "**Objective:**  \n",
    "Develop a <u>Generative Adversarial Network (GAN) model</u> for <u>image generation</u>, utilizing the <u>CIFAR10 dataset</u>. The model aims to generate <u>1000 high-quality, small color images</u> in <u>10 distinct classes</u>, showcasing its ability to learn and replicate complex visual patterns.\n",
    "\n",
    "**Background:**  \n",
    "GANs are a revolutionary class of artificial neural networks used in unsupervised machine learning tasks. They consist of two parts: a Generator, which creates images, and a Discriminator, which evaluates them. The objective is to train a GAN that excels in producing diverse, realistic images that closely mimic the characteristics of the CIFAR10 dataset.\n",
    "\n",
    "**Key Features:** <br>\n",
    "Implement and evaluate different GAN architectures to determine the most effective model for the CIFAR10 specific image generation task, which should generate images that not only are visually appealing and realistic but also display a wide range of creativity within the constraints of the 10 classes in the dataset.\n",
    "\n",
    "**Output Specification:**  \n",
    "The model will produce images that are evaluated based on their similarity to the real images in the CIFAR10 dataset and their diversity across the dataset's classes. The performance of the GAN will be a crucial indicator of its effectiveness in learning and replicating complex patterns from a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font size = 5>Performing initial set-up</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check GPU is available\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "# Memory control: Prevent tensorflow from allocating totality of GPU memory\n",
    "for gpu in gpus:\n",
    "    print(tf.config.experimental.get_device_details(gpu))\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(f\"There are {len(gpus)} GPU(s) present.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=UserWarning)     \n",
    "simplefilter(action='ignore', category=FutureWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.keras.utils.set_random_seed(0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font size = 5>Background Research</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIFAR10 Dataset:**  \n",
    "- The CIFAR10 dataset consists of <u>60,000 colour images</u> in <u>10 classes</u>. \n",
    "- There are 6,000 images per class.\n",
    "\n",
    "**Images:**  \n",
    "- The images are split into 50,000 train images and 10,000 test images.\n",
    "- The images are of <u>size 32x32</u>.\n",
    "\n",
    "**Classes:**  \n",
    "- Total of 10 distinct classes:\n",
    "    1. airplane\n",
    "    2. automoblie\n",
    "    3. bird\n",
    "    4. cat\n",
    "    5. deer\n",
    "    6. dog\n",
    "    7. frog\n",
    "    8. horse\n",
    "    9. ship\n",
    "    10. truck\n",
    "- Classes are mutually exclusive. \n",
    "    - There is no overlap between automobiles and trucks, neither includes pickup trucks.\n",
    "    - \"Automobile\" includes sedans, SUVs, etc. \n",
    "    - \"Truck\" includes only big trucks.\n",
    "\n",
    "**Batches:**  \n",
    "- The dataset is divided into 5 train batches & 1 test batch, each with 10,000 images.\n",
    "- Train batches contain 50,000 images in total from each class in random order\n",
    "    - Some batches contain more images from one class than another\n",
    "- Test batch contains 10,000 randomly-selected images from each class\n",
    "\n",
    "**Source:**\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>Load CIFAR10 Dataset</font>\n",
    "\n",
    "Returns two tuples, each containing two elements. <br>\n",
    "The first element of each tuple is an array of images.<br>\n",
    "The second element is an array of corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device_name': 'NVIDIA GeForce RTX 3050 Laptop GPU', 'compute_capability': (8, 6)}\n",
      "There are 1 GPU(s) present.\n"
     ]
    }
   ],
   "source": [
    "# Load CIFAR10 Dataset\n",
    "cifar10 = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train & test\n",
    "(x_train, y_train), (x_test, y_test) = cifar10\n",
    "\n",
    "print(f\"X_train shape: {x_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {x_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>Class Labels</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map integer class labels to their corresponding class names \n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font size = 5>Exploratory Data Analysis (EDA)</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA) Tips\n",
    "1. *Visualize Images*: Display a sample of images from each of the 10 classes to understand the variety and quality of images in the dataset.\n",
    "2. *Class Distribution*: Check if the dataset is balanced across different classes.\n",
    "3. *Pixel Distribution*: Analyze the distribution of pixel values across the entire dataset and within each class to understand the color distribution.\n",
    "4. *Image Size and Resolution Analysis*: CIFAR-10 images are 32x32, but it's good to visualize and understand this small resolution's impact.\n",
    "5. *Correlation Analysis*: Assess if there are any interesting correlations between different color channels.\n",
    "6. *Data Augmentation Impact*: Visualize how different data augmentation techniques (like rotation, flipping, scaling) affect the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>Data Visualization</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 5>Class Distribution</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font size = 5>Feature Engineering</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "1. *Normalization*: Scale pixel values to a range (e.g., 0 to 1 or -1 to 1) to help the model train more efficiently.\n",
    "2. *Color Space Conversion*: Experiment with different color spaces (like HSV or grayscale) to see if they impact the GAN's performance.\n",
    "3. *Edge Detection*: Implement edge detection (like the Sobel filter) to create features that emphasize shapes and borders in images.\n",
    "4. *Noise Addition*: For robustness, you can add small amounts of noise to the input images.\n",
    "5. *Data Augmentation*: Use techniques like flipping, cropping, or rotating to artificially expand the training dataset.\n",
    "6. *Dimensionality Reduction*: Experiment with techniques like PCA to reduce the number of input features while retaining essential information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font size = 5>Initial Modelling</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Models:\n",
    "1. DCGAN\n",
    "2. cGAN\n",
    "3. SAGAN\n",
    "\n",
    "Early Stopping to identify when the model collapses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font size = 5>Model Improvement</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous\n",
    "- *Hyperparameter Tuning*: Experiment with different architectures and training parameters.\n",
    "- *Loss Function Analysis*: Explore various loss functions and their impact on the generated images' quality\n",
    "\n",
    "### Advanced Techniques\n",
    "1. *Feature Learning with Autoencoders*: Use autoencoders to learn compressed representations of the images, which might help in generating new images.\n",
    "2. *Generative Feature Extraction*: Experiment with using features extracted from other pretrained models as inputs to your GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<font size = 5>Result Evaluation</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "- *FID Score*: Use the Fréchet Inception Distance to evaluate the quality of images generated by your GAN.\n",
    "- *Visual Inspection*: Regularly sample and inspect generated images to qualitatively assess the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
